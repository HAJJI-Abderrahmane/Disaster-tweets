{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1936,
     "status": "ok",
     "timestamp": 1610099410784,
     "user": {
      "displayName": "Abderrahmane HAJJI",
      "photoUrl": "",
      "userId": "15675344311234361853"
     },
     "user_tz": -60
    },
    "id": "hBOtU7Cpz8fh",
    "outputId": "071eb475-2043-403a-e994-e449eb6b52f7"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6cd94ce372dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#### preprocessing should exists in the repo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\test3\\Disaster tweets\\preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;31m# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconvert_abbrev_in_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "#### preprocessing should exists in the repo\n",
    "import preprocessing\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQywp0WXz341"
   },
   "outputs": [],
   "source": [
    "###These are setups that we ran and each is slightly different than the other, you can play with it as well\n",
    "# version 15\n",
    "# convert_abbrev_flag = False\n",
    "# model_type = 'default'\n",
    "# Dropout_num = 0\n",
    "# learning_rate = 1e-5\n",
    "# max_len = 160\n",
    "# layers = [] #not including final layer\n",
    "# activation = 'relu' #for the non-final layers\n",
    "# remove_emoji_flag = False\n",
    "# remove_URL_flag = False\n",
    "# remove_html_flag = False\n",
    "# remove_punct_flag = False\n",
    "# epochs=3\n",
    "# batch_size=16\n",
    "# validation_split=0.2\n",
    "# remove_rows_based_on_cv = True #remove rows based on cross val predictions from train set\n",
    "# remove_row_cutoffs = (0.4,0.6) #if cutoffs for class 0 and 1\n",
    "# model_name = 'bert_default_dropout_{}_shape_{}'.format(Dropout_num,'_'.join([str(i) for i in layers]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5uJbO6dBwL6"
   },
   "outputs": [],
   "source": [
    "# version 16\n",
    "# convert_abbrev_flag = False\n",
    "# model_type = 'dropout'\n",
    "# Dropout_num = 0.5\n",
    "# learning_rate = 1e-5\n",
    "# max_len = 160\n",
    "# layers = [] #not including final layer\n",
    "# activation = 'relu' #for the non-final layers\n",
    "# remove_emoji_flag = False\n",
    "# remove_URL_flag = False\n",
    "# remove_html_flag = False\n",
    "# remove_punct_flag = False\n",
    "# epochs=3\n",
    "# batch_size=16\n",
    "# validation_split=0.2\n",
    "# remove_rows_based_on_cv = True #remove rows based on cross val predictions from train set\n",
    "# remove_row_cutoffs = (0.4,0.6) #if cutoffs for class 0 and 1\n",
    "# model_name = 'bert_default_dropout_{}_shape_{}'.format(Dropout_num,'_'.join([str(i) for i in layers]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lj1kPSFDMmAk"
   },
   "outputs": [],
   "source": [
    "# version 17\n",
    "# convert_abbrev_flag = False\n",
    "# model_type = 'dropout'\n",
    "# Dropout_num = 0.5\n",
    "# learning_rate = 1e-5\n",
    "# max_len = 160\n",
    "# layers = [] #not including final layer\n",
    "# activation = 'relu' #for the non-final layers\n",
    "# remove_emoji_flag = True\n",
    "# remove_URL_flag = True\n",
    "# remove_html_flag = True\n",
    "# remove_punct_flag = True\n",
    "# epochs=3\n",
    "# batch_size=16\n",
    "# validation_split=0.2\n",
    "# remove_rows_based_on_cv = True #remove rows based on cross val predictions from train set\n",
    "# remove_row_cutoffs = (0.4,0.6) #if cutoffs for class 0 and 1\n",
    "# model_name = 'bert_default_dropout_{}_shape_{}'.format(Dropout_num,'_'.join([str(i) for i in layers]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIcfX8M-Y8Gf"
   },
   "outputs": [],
   "source": [
    "# version 18\n",
    "convert_abbrev_flag = False\n",
    "model_type = 'dropout'\n",
    "Dropout_num = 0.5\n",
    "learning_rate = 1e-5\n",
    "max_len = 160\n",
    "layers = [] #not including final layer\n",
    "activation = 'relu' #for the non-final layers\n",
    "remove_emoji_flag = True\n",
    "remove_URL_flag = False\n",
    "remove_html_flag = False\n",
    "remove_punct_flag = False\n",
    "epochs=3\n",
    "batch_size=16\n",
    "validation_split=0.2\n",
    "remove_rows_based_on_cv = True #remove rows based on cross val predictions from train set\n",
    "remove_row_cutoffs = (0.4,0.6) #if cutoffs for class 0 and 1\n",
    "model_name = 'bert_default_dropout_{}_shape_{}'.format(Dropout_num,'_'.join([str(i) for i in layers]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMGVu8c9z344"
   },
   "source": [
    "# Load and Preprocess\n",
    "\n",
    "- Load CSV files containing training data\n",
    "- Do cleaning if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPX_84msz345"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/data/train.csv\")\n",
    "test = pd.read_csv(\"/data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cqxhO4hz345"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if convert_abbrev_flag:\n",
    "    train[\"TweetText\"] = train[\"TweetText\"].apply(lambda x: preprocessing.convert_abbrev_in_text(x))\n",
    "    test[\"TweetText\"] = test[\"TweetText\"].apply(lambda x: preprocessing.convert_abbrev_in_text(x))\n",
    "    \n",
    "if remove_emoji_flag:\n",
    "    train[\"TweetText\"] = train[\"TweetText\"].apply(lambda x: preprocessing.remove_emoji(x))\n",
    "    test[\"TweetText\"] = test[\"TweetText\"].apply(lambda x: preprocessing.remove_emoji(x))\n",
    "\n",
    "if remove_URL_flag:\n",
    "    train['TweetText'] = train['TweetText'].apply(preprocessing.remove_URL)\n",
    "    test['TweetText'] = test['TweetText'].apply(preprocessing.remove_URL)\n",
    "if remove_html_flag:\n",
    "    train['TweetText'] = train['TweetText'].apply(preprocessing.remove_html)\n",
    "    test['TweetText'] = test['TweetText'].apply(preprocessing.remove_html)\n",
    "if remove_punct_flag:\n",
    "    train['TweetText'] = train['TweetText'].apply(preprocessing.remove_punct)\n",
    "    test['TweetText'] = test['TweetText'].apply(preprocessing.remove_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2hg_1Zez346"
   },
   "source": [
    "# BERT Model\n",
    "\n",
    "- Load BERT from the Tensorflow Hub\n",
    "- Load CSV files containing training data\n",
    "- Load tokenizer from the bert layer\n",
    "- Encode the text into tokens, masks, and segment flags\n",
    "- build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6671,
     "status": "ok",
     "timestamp": 1610099426206,
     "user": {
      "displayName": "Abderrahmane HAJJI",
      "photoUrl": "",
      "userId": "15675344311234361853"
     },
     "user_tz": -60
    },
    "id": "XIqnQ2Dpz346",
    "outputId": "5af64922-6298-4d95-ae99-eb80f0428c43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "# !pip install sentencepiece\n",
    "import sentencepiece\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFopMpuEz346"
   },
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer,max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n",
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    \n",
    "    if model_type=='default':\n",
    "        # Without Dropout\n",
    "        for layer in layers:\n",
    "            clf_output = Dense(layer, activation=activation)(clf_output)\n",
    "        out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    elif model_type=='dropout':\n",
    "        # With Dropout(Dropout_num), Dropout_num > 0\n",
    "        for layer in layers:\n",
    "            x = Dropout(Dropout_num)(clf_output)\n",
    "            clf_output = Dense(layer, activation=activation)(x)\n",
    "        x = Dropout(Dropout_num)(clf_output)\n",
    "        out = Dense(1, activation='sigmoid')(x)\n",
    "    elif model_type=='GlobalAveragePooling1D':\n",
    "        for layer in layers:\n",
    "            if Dropout_num>0:\n",
    "                clf_output = Dropout(Dropout_num)(clf_output)\n",
    "            clf_output = Dense(layer, activation=activation)(clf_output)\n",
    "        x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
    "        out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280003,
     "status": "ok",
     "timestamp": 1610099706220,
     "user": {
      "displayName": "Abderrahmane HAJJI",
      "photoUrl": "",
      "userId": "15675344311234361853"
     },
     "user_tz": -60
    },
    "id": "ArBgvyywz346",
    "outputId": "f836f868-6e21-4be9-f646-4aa4c3aabef6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting BERT module time :279.0259644985199\n",
      "CPU times: user 42.6 s, sys: 10.6 s, total: 53.3 s\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "start=time.time()\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"#/2 is the updated version. need to try with that\n",
    "# module_url = \"https://tfhub.dev/tensorflow/albert_en_base/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
    "end = time.time()\n",
    "print(\"Getting BERT module time :{}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oU0cMt1wz347"
   },
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283316,
     "status": "ok",
     "timestamp": 1610099709554,
     "user": {
      "displayName": "Abderrahmane HAJJI",
      "photoUrl": "",
      "userId": "15675344311234361853"
     },
     "user_tz": -60
    },
    "id": "6odJUe4e2tzK",
    "outputId": "a2a07898-17fa-4464-d8cb-83db16c2082d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6525, 160) (2610, 160)\n"
     ]
    }
   ],
   "source": [
    "train_input = bert_encode(train.TweetText.values, tokenizer, max_len=max_len)\n",
    "test_input = bert_encode(test.TweetText.values, tokenizer, max_len=max_len)\n",
    "train_labels = train.Label.apply(lambda x : 0 if x==\"Sports\" else 1).values\n",
    "\n",
    "print(train_input[0].shape,test_input[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cl2q6NW2z347"
   },
   "outputs": [],
   "source": [
    "train_input = bert_encode(train.text.values, tokenizer, max_len=max_len)\n",
    "test_input = bert_encode(test.text.values, tokenizer, max_len=max_len)\n",
    "train_labels = train.target.values\n",
    "\n",
    "print(train_input[0].shape,test_input[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 284614,
     "status": "ok",
     "timestamp": 1610099710863,
     "user": {
      "displayName": "Abderrahmane HAJJI",
      "photoUrl": "",
      "userId": "15675344311234361853"
     },
     "user_tz": -60
    },
    "id": "5NRWGQvkz347",
    "outputId": "d4406a51-d3dc-4a14-f723-898813546d9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 1024)         0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1024)         0           tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=160)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2113632,
     "status": "ok",
     "timestamp": 1610101539898,
     "user": {
      "displayName": "Abderrahmane HAJJI",
      "photoUrl": "",
      "userId": "15675344311234361853"
     },
     "user_tz": -60
    },
    "id": "YEgQfNc3z347",
    "outputId": "5bf528d5-d825-47e2-a23e-614f2d9c021c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "327/327 [==============================] - 577s 2s/step - loss: 0.2479 - accuracy: 0.8772 - val_loss: 0.1064 - val_accuracy: 0.9556\n",
      "Epoch 2/3\n",
      "327/327 [==============================] - 552s 2s/step - loss: 0.0766 - accuracy: 0.9698 - val_loss: 0.0936 - val_accuracy: 0.9670\n",
      "Epoch 3/3\n",
      "327/327 [==============================] - 552s 2s/step - loss: 0.0185 - accuracy: 0.9929 - val_loss: 0.1267 - val_accuracy: 0.9670\n",
      "Training the shit time :1829.0113744735718\n"
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint = ModelCheckpoint('bertwithdropout2.h5', monitor='val_loss', save_best_only=True)\n",
    "start=time.time()\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "end = time.time()\n",
    "print(\"Training the stuff time :{}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 131802,
     "status": "ok",
     "timestamp": 1610101671720,
     "user": {
      "displayName": "Abderrahmane HAJJI",
      "photoUrl": "",
      "userId": "15675344311234361853"
     },
     "user_tz": -60
    },
    "id": "eoPJUZswz347",
    "outputId": "d5b6ab2a-ae54-4dd8-95ab-d03259caf8e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the models:130.72085332870483\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model.load_weights('bertwithdropout2.h5')\n",
    "test_pred = model.predict(test_input)\n",
    "end = time.time()\n",
    "print(\"loading the models:{}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYckksx8_5Kz"
   },
   "outputs": [],
   "source": [
    "def couldbebetter(aa):\n",
    "  ree=[]\n",
    "  for i in aa:\n",
    "    if i==0:\n",
    "      ree.append(\"Sports\")\n",
    "    else:\n",
    "      ree.append(\"Politics\")\n",
    "  return ree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNjfA5i4AV6W"
   },
   "outputs": [],
   "source": [
    "datalabel=np.array(couldbebetter(test_pred.round().astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZUMCuQCAjYU"
   },
   "outputs": [],
   "source": [
    "datatweetid=test.TweetId"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Last(Best).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
